Hadoop: 


Sqoop:
Import from SQL Tables:
Import from Hive Tables:



SparkContext: 
1. To read DF(Data Frames) - we need to use sqlContext.
2. We can use SparkContext or SQLContext to read data. when input is textfile format so using sc.


Save Data as 
Avro File: 
    Save data in special format as avro. Convert data to DF, use APIs on top of DF and save as avro.

Text/Parquet:


Spark SQL:



Streaming Analytics:


Flume:


Kafka:







@Streaming Analytics using Flume Kafka and Spark Streaming
134  # 01 Streaming Analytics - Introduction
Spark Streaming alternatives at times - Flink, Storm
Flink and Storm not a part of certification.


@Flume Different implementations of agents
142  # 09 Streaming Analytics - Flume - Different implementations of agents


@Kafka High level architecture
143  # 10 Streaming Analytics - Kafka - High level architecture


@Spark Streaming
147  # 14 Spark Streaming - Getting Started


@Kafka and Spark Streaming
161  # 30 Kafka and Spark Streaming - Department Wise Count - Run and validate application


@End of CCA175 relevant topics.
